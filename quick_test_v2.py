# /quick_test_v2.py
"""
CogniQuantum V2 ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å•é¡Œã®è¨ºæ–­ã¨åŸºæœ¬å‹•ä½œç¢ºèªç”¨
"""
import asyncio
import json
import logging
import os
import sys
from typing import Dict, Any

# ãƒ‘ã‚¹ã®è¨­å®š
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def check_ollama_status():
    """Ollamaã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("ğŸ” OllamaçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ä¸­...")
    
    try:
        import httpx
        async with httpx.AsyncClient(timeout=5.0) as client:
            try:
                response = await client.get("http://localhost:11434/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    models = [model['name'] for model in data.get('models', [])]
                    print(f"âœ… Ollamaã‚µãƒ¼ãƒãƒ¼: æ¥ç¶šOK")
                    print(f"ğŸ“¦ åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {models}")
                    return True, models
                else:
                    print(f"âŒ Ollamaã‚µãƒ¼ãƒãƒ¼: HTTP {response.status_code}")
                    return False, []
            except Exception as e:
                print(f"âŒ Ollamaã‚µãƒ¼ãƒãƒ¼: æ¥ç¶šå¤±æ•— ({e})")
                return False, []
    except ImportError:
        print("âŒ httpx ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False, []

async def test_basic_functionality():
    """åŸºæœ¬æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆä¸­...")
    
    try:
        from llm_api.providers import list_providers, list_enhanced_providers
        
        standard = list_providers()
        enhanced = list_enhanced_providers()
        
        print(f"ğŸ“‹ æ¨™æº–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {standard}")
        print(f"ğŸ“‹ æ‹¡å¼µãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ V1: {enhanced.get('v1', [])}")
        print(f"ğŸ“‹ æ‹¡å¼µãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ V2: {enhanced.get('v2', [])}")
        
        return True
    except Exception as e:
        print(f"âŒ åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

async def test_provider_creation():
    """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ­ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆä¸­...")
    success = True
    
    try:
        from llm_api.providers import get_provider
        
        # æ¨™æº–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ
        try:
            provider = get_provider('ollama', enhanced=False)
            print("âœ… æ¨™æº–Ollamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: ä½œæˆæˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨™æº–Ollamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {e}")
            success = False
        
        # æ‹¡å¼µãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆï¼ˆV2å„ªå…ˆï¼‰
        try:
            # 'prefer_v2'å¼•æ•°ã‚’å‰Šé™¤ã—ã¦ä¿®æ­£
            provider = get_provider('ollama', enhanced=True)
            print("âœ… æ‹¡å¼µOllamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: ä½œæˆæˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ‹¡å¼µOllamaãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {e}")
            success = False
        
        return success
    except Exception as e:
        print(f"âŒ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­ã«å¤±æ•—: {e}")
        return False

async def test_simple_call():
    """ã‚·ãƒ³ãƒ—ãƒ«ãªå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“ ã‚·ãƒ³ãƒ—ãƒ«å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆä¸­...")
    
    # ã“ã®ãƒ†ã‚¹ãƒˆã¯OllamaãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã«ã®ã¿å®Ÿè¡Œ
    ollama_ok, models = await check_ollama_status()
    if not ollama_ok or not models:
        print("âš ï¸  Ollamaåˆ©ç”¨ä¸å¯ã®ãŸã‚ã€å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
        # ã‚¹ã‚­ãƒƒãƒ—ã¯å¤±æ•—ã§ã¯ãªã„ã®ã§Trueã‚’è¿”ã™
        return True
    
    try:
        from llm_api.providers import get_provider
        
        selected_model = models[0].split(':')[0]
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {selected_model}")
        
        provider = get_provider('ollama', enhanced=False)
        response = await provider.call(
            "Hello, respond with just 'OK'",
            model=selected_model
        )
        
        if response.get('text'):
            print(f"âœ… å‘¼ã³å‡ºã—æˆåŠŸ: {response['text'][:50]}...")
            return True
        else:
            print(f"âŒ å‘¼ã³å‡ºã—å¤±æ•—: {response.get('error', 'ç©ºã®å¿œç­”')}")
            return False
            
    except Exception as e:
        print(f"âŒ å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

def show_setup_guide():
    """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ã®è¡¨ç¤º"""
    print("""
ğŸ”§ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰:

1. Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨èµ·å‹•:
   curl -fsSL https://ollama.ai/install.sh | sh
   ollama serve

2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ«:
   ollama pull gemma3:latest    # æ¨å¥¨
   ollama pull llama3.1      # è»½é‡ç‰ˆ

3. ç¢ºèª:
   ollama list

4. ãƒ†ã‚¹ãƒˆå†å®Ÿè¡Œ:
   python quick_test_v2.py
""")

def show_troubleshooting():
    """ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æƒ…å ±"""
    print("""
ğŸš¨ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:

ã€Ollamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ãªã„ã€‘
- ãƒãƒ¼ãƒˆ11434ãŒä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
- åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§: ollama serve

ã€ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€‘
- ollama list ã§ç¢ºèª
- ollama pull <ãƒ¢ãƒ‡ãƒ«å> ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

ã€ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¨ãƒ©ãƒ¼ã€‘
- Pythonä¾å­˜é–¢ä¿‚ã‚’ç¢ºèª: pip install -r requirements.txt
- ãƒ‘ã‚¹ã®ç¢ºèª: export PYTHONPATH=.

ã€ãã®ä»–ã€‘
- ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«å¤‰æ›´: export LOG_LEVEL=DEBUG
- è©³ç´°æƒ…å ±: python quick_test_v2.py --verbose
""")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="CogniQuantum V2 ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°")
    parser.add_argument("--setup-guide", action="store_true", help="ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰è¡¨ç¤º")
    parser.add_argument("--troubleshooting", action="store_true", help="ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¡¨ç¤º")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    if args.setup_guide:
        show_setup_guide()
        return
    
    if args.troubleshooting:
        show_troubleshooting()
        return
    
    print("ğŸš€ CogniQuantum V2 ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    tests = [
        ("OllamaçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯", check_ollama_status),
        ("åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ", test_basic_functionality),
        ("ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ", test_provider_creation),
        ("ã‚·ãƒ³ãƒ—ãƒ«å‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ", test_simple_call)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            # check_ollama_statusã¯ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™ã®ã§ç‰¹åˆ¥æ‰±ã„
            if test_name == "OllamaçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯":
                result, _ = await test_func()
                results.append((test_name, result))
            else:
                result = await test_func()
                results.append((test_name, result))
        except Exception as e:
            logger.error(f"{test_name}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            results.append((test_name, False))
    
    print("\n" + "=" * 50)
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
    
    passed = sum(1 for _, success in results if success)
    total = len(results)

    for test_name, success in results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"  {status} {test_name}")
    
    print(f"\nğŸ“ˆ ç·åˆçµæœ: {passed}/{total} ãƒ†ã‚¹ãƒˆåˆæ ¼")
    
    if passed == total:
        print("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼ï¼ã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã—ã¾ã™ã€‚")
        print("\næ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆã‚’è¡Œã£ã¦ãã ã•ã„:")
        print("python fetch_llm_v2.py ollama 'Hello' --mode simple")
    elif not results[0][1]: # Ollamaãƒã‚§ãƒƒã‚¯ãŒå¤±æ•—ã—ãŸå ´åˆ
        print("ğŸ˜ Ollamaã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print("python quick_test_v2.py --setup-guide")
    else:
        print("âš ï¸  ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        print("python quick_test_v2.py --troubleshooting")

if __name__ == "__main__":
    asyncio.run(main())